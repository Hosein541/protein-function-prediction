{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predicting Protein Function: A Comparison of CNN and Transformer Models\n",
        "\n",
        "* **Author:** Hosein Mohammadi\n",
        "* **Date:** July 2024\n",
        "* **Contact:** [huseinmohammadi83@gmail.com](mailto:huseinmohammadi83@gmail.com)\n",
        "* **LinkedIn:** [Hosein Mohammadi](https://www.linkedin.com/in/hosein-mohammadi-979b8a2b2/)\n",
        "* **Project Repository:** [protein-function-prediction](https://github.com/Hosein541/protein-function-prediction)\n",
        "\n",
        "---\n",
        "\n",
        "### Project Overview\n",
        "\n",
        "This notebook contains the complete code for the \"Protein Function Prediction\" project. The primary goal is to predict a protein's functions (represented by Gene Ontology terms) directly from its amino acid sequence.\n",
        "\n",
        "To achieve this, we implement and rigorously compare two distinct deep learning architectures:\n",
        "\n",
        "1.  A **1D Convolutional Neural Network (CNN)** built from scratch in Keras to serve as a strong baseline.\n",
        "2.  A fine-tuned **Transformer model (ESM-2)** from Hugging Face, leveraging the power of transfer learning from a model pre-trained on millions of protein sequences.\n",
        "\n",
        "The notebook covers all steps from data acquisition and preprocessing (using data from UniProt) to model training, hyperparameter optimization (threshold tuning), and a final comparative analysis of the results.1mm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =======================================================\n",
        "#                      IMPORTS\n",
        "# =======================================================\n",
        "\n",
        "# --- Standard Libraries ---\n",
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# --- Data Manipulation & Plotting ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Scikit-Learn (General ML & Metrics) ---\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.metrics import f1_score, roc_auc_score, classification_report\n",
        "\n",
        "# --- TensorFlow / Keras (for the CNN model) ---\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# --- PyTorch & Hugging Face (for the Transformer model) ---\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "\n",
        "# --- Environment Setup ---\n",
        "# Disable W&B logging for a cleaner run\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Data Loading and Preprocessing\n",
        "\n",
        "In this first step, we load the dataset sourced from UniProt and perform the initial cleaning required to prepare it for our models.\n",
        "\n",
        "The process includes:\n",
        "* **Loading Data:** The `.tsv.gz` file is loaded into a pandas DataFrame.\n",
        "* **Initial Inspection:** We use `.head()` and `.info()` to get a first look at the data structure and identify any missing values.\n",
        "* **Cleaning:**\n",
        "    * We remove any rows that are missing a protein sequence or Gene Ontology (GO) term, as they are unusable for training.\n",
        "    * We remove any proteins with duplicate sequences to prevent data redundancy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnhJdmt9-u8X",
        "outputId": "19674807-e96a-466a-da55-1aa9109fbda8"
      },
      "outputs": [],
      "source": [
        "# 1. Place the name of the file you downloaded from UniProt here\n",
        "file_path = \"uniprotkb_reviewed_true_AND_organism_id_2025_07_29.tsv.gz\"\n",
        "\n",
        "# 2. Load data\n",
        "# Since our file is tab-separated, we use sep='\\t'\n",
        "try:\n",
        "    df = pd.read_csv(file_path, sep='\\t')\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File '{file_path}' not found. Please check the file name and path.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# 3. Display the first 5 rows for initial inspection\n",
        "print(\"--- First 5 rows of data ---\")\n",
        "print(df.head())\n",
        "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
        "\n",
        "# 4. Display general DataFrame information (important)\n",
        "# This section tells us how many rows of data we have and whether there are any empty (NaN) values\n",
        "print(\"--- General DataFrame information (before cleaning) ---\")\n",
        "df.info()\n",
        "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
        "\n",
        "\n",
        "# 5. Initial cleaning\n",
        "# Remove rows that have empty values in the 'Sequence' or 'Gene Ontology (GO)' columns\n",
        "df.dropna(subset=['Sequence', 'Gene Ontology (GO)'], inplace=True)\n",
        "\n",
        "# Remove duplicate rows based on protein sequence\n",
        "# Sometimes different proteins have identical sequences, which are considered duplicates for our model\n",
        "initial_rows = len(df)\n",
        "df.drop_duplicates(subset=['Sequence'], inplace=True)\n",
        "print(f\"{initial_rows - len(df)} duplicate rows removed.\")\n",
        "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
        "\n",
        "\n",
        "# 6. Display final information after cleaning\n",
        "print(\"--- General DataFrame information (after cleaning) ---\")\n",
        "print(\"Final DataFrame shape:\", df.shape)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Label Processing: From Text to Binary Vectors\n",
        "\n",
        "This section transforms the raw `Gene Ontology (GO)` text column into a machine-learning-ready format. The goal is to create a binary matrix where each row represents a protein and each column represents a specific function.\n",
        "\n",
        "This process involves several key steps:\n",
        "\n",
        "1.  **Extract GO Codes:** We use **regular expressions** to parse the text and extract all valid GO codes (e.g., `GO:0005737`).\n",
        "2.  **Identify Top Labels:** To make the classification problem manageable, we count all the extracted GO terms and select the **1,000 most common** ones to act as our target labels. The number of labels is a **hyperparameter** that can be tuned.\n",
        "3.  **Filter the Dataset:** We remove any proteins that do not have at least one of these top 1,000 GO terms.\n",
        "4.  **Binarize Labels:** We use Scikit-Learn's **`MultiLabelBinarizer`** to convert the list of GO terms for each protein into a binary vector (a process also known as one-hot encoding). A `1` in a column indicates the protein has that function, and a `0` indicates it does not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOzpeduR_EiQ",
        "outputId": "63d0be8c-49b2-4b33-d679-2127c224d092"
      },
      "outputs": [],
      "source": [
        "# 1. Extract GO codes from text string\n",
        "def extract_go_terms(text):\n",
        "    # This regex finds all strings that start with \"GO:\" and have 7 digits\n",
        "    return re.findall(r'GO:\\d{7}', str(text))\n",
        "\n",
        "print(\"Extracting GO codes...\")\n",
        "df['go_terms'] = df['Gene Ontology (GO)'].apply(extract_go_terms)\n",
        "\n",
        "# Remove rows where no valid GO code was found\n",
        "df = df[df['go_terms'].apply(len) > 0]\n",
        "print(f\"Number of remaining proteins after removing rows without GO: {len(df)}\")\n",
        "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
        "\n",
        "\n",
        "# 2. Find the most common GO terms\n",
        "# Collect all lists of GO codes into one large list\n",
        "all_go_terms = [term for terms_list in df['go_terms'] for term in terms_list]\n",
        "\n",
        "# Count the occurrences of each GO code\n",
        "go_counts = Counter(all_go_terms)\n",
        "\n",
        "# Determine the number of most common labels (e.g., the first 1000)\n",
        "# This number is a hyperparameter and can be changed\n",
        "NUM_LABELS = 1000\n",
        "top_go_terms = [term for term, count in go_counts.most_common(NUM_LABELS)]\n",
        "print(f\"Total number of unique GO codes: {len(go_counts)}\")\n",
        "print(f\"{NUM_LABELS} most common GO codes selected for modeling.\")\n",
        "print(\"Example of common codes:\", top_go_terms[:10])\n",
        "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
        "\n",
        "\n",
        "# 3. Filter DataFrame\n",
        "# Keep only proteins that have at least one of the most common labels\n",
        "def filter_top_go(terms):\n",
        "    return [term for term in terms if term in top_go_terms]\n",
        "\n",
        "df['go_terms_filtered'] = df['go_terms'].apply(filter_top_go)\n",
        "df = df[df['go_terms_filtered'].apply(len) > 0]\n",
        "print(f\"Number of final proteins for modeling: {len(df)}\")\n",
        "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
        "\n",
        "\n",
        "# 4. Convert to binary format (Multi-Label Binarization)\n",
        "print(\"Converting labels to binary format...\")\n",
        "mlb = MultiLabelBinarizer(classes=top_go_terms)\n",
        "y = mlb.fit_transform(df['go_terms_filtered'])\n",
        "\n",
        "# Convert the binary matrix to a DataFrame for better viewing\n",
        "labels_df = pd.DataFrame(y, columns=mlb.classes_)\n",
        "\n",
        "print(\"Shape of the labels matrix (number of samples, number of labels):\", labels_df.shape)\n",
        "print(\"Example of the final labels matrix:\")\n",
        "print(labels_df.head())\n",
        "\n",
        "# Now 'df' contains sequences and 'labels_df' contains labels ready for the model.\n",
        "# You can save these two for subsequent steps.\n",
        "# df.to_csv('processed_sequences.csv', index=False)\n",
        "# labels_df.to_csv('processed_labels.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Sequence Preparation: Tokenization and Padding\n",
        "\n",
        "In this step, we convert the protein sequences from text strings into fixed-size numerical matrices that can be fed into a deep learning model. This is a standard procedure in sequence-based modeling.\n",
        "\n",
        "The process involves two main parts:\n",
        "\n",
        "1.  **Tokenization:** First, we create a **vocabulary** that maps each unique amino acid character to an integer. We then use this vocabulary to convert each protein sequence into a list of numbers.\n",
        "\n",
        "2.  **Padding:** Deep learning models require their inputs to be of a uniform size. Since protein sequences have varying lengths, we standardize them. We calculate the **95th percentile** of all sequence lengths and use this as our maximum length (`MAX_LEN`). Sequences longer than this are truncated, and shorter sequences are \"padded\" with a special token (0) until they reach this length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qcffzhsd_e4n",
        "outputId": "f64fe469-e63a-41bb-ed63-df7bdb8e02c7"
      },
      "outputs": [],
      "source": [
        "# 1. Build dictionary for amino acids\n",
        "# List of all standard amino acids + a token for unknown amino acids ('U')\n",
        "# and a token for padding ('X') which takes the value zero\n",
        "amino_acids = 'ACDEFGHIKLMNPQRSTVWYU'\n",
        "vocab = {aa: i + 1 for i, aa in enumerate(amino_acids)}\n",
        "vocab['X'] = 0  # Padding token\n",
        "\n",
        "VOCAB_SIZE = len(vocab)\n",
        "print(f\"Vocabulary Size: {VOCAB_SIZE}\")\n",
        "print(\"Amino acid dictionary:\", vocab)\n",
        "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
        "\n",
        "\n",
        "# 2. Convert sequences to numerical lists (tokenization)\n",
        "def tokenize_sequence(sequence):\n",
        "    return [vocab.get(aa, vocab['U']) for aa in sequence] # If amino acid is not found, consider it 'U'\n",
        "\n",
        "print(\"Tokenizing sequences...\")\n",
        "df['sequence_tokenized'] = df['Sequence'].apply(tokenize_sequence)\n",
        "print(\"Example of a tokenized sequence:\")\n",
        "print(df['sequence_tokenized'].iloc[0])\n",
        "print(df['Sequence'].iloc[0])\n",
        "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
        "\n",
        "\n",
        "# 3. Standardize sequence length (Padding)\n",
        "# Find a suitable length for all sequences (e.g., 95th percentile)\n",
        "# This prevents excessive length due to a few very long sequences\n",
        "MAX_LEN = int(df['Sequence'].str.len().quantile(0.95))\n",
        "print(f\"Maximum length for padding (95th percentile): {MAX_LEN}\")\n",
        "\n",
        "# Apply padding\n",
        "# pre: padding is added to the beginning of the sequence\n",
        "X = pad_sequences(df['sequence_tokenized'], maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "\n",
        "print(\"Final input matrix shape (number of samples, sequence length):\", X.shape)\n",
        "print(\"Example of a sequence after padding:\")\n",
        "print(X[0])\n",
        "\n",
        "# Now matrix 'X' contains inputs ready for the model and matrix 'y' (from previous step) are our outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Baseline Model: 1D-CNN\n",
        "\n",
        "With the data fully prepared, we now define, compile, and train our first model. This **1D-Convolutional Neural Network (CNN)** will serve as our **baseline**, providing a performance benchmark that our more advanced Transformer model will need to surpass.\n",
        "\n",
        "The process involves these steps:\n",
        "\n",
        "* **Data Splitting:** We first divide our data into training (80%) and testing (20%) sets to ensure we can evaluate the model's performance on unseen data.\n",
        "\n",
        "* **Model Architecture:** We build the model using the Keras `Sequential` API. The key layers are:\n",
        "    * An **`Embedding`** layer, which learns a dense vector representation for each amino acid.\n",
        "    * A **`Conv1D`** layer, which is highly effective at scanning for and learning local patterns (motifs) within the protein sequences.\n",
        "    * A final **`Dense`** output layer with a **`sigmoid`** activation function, which is essential for multi-label classification as it allows the model to predict a probability for each of the 1,000 labels independently.\n",
        "\n",
        "* **Compilation:** The model is compiled using the `adam` optimizer and the **`binary_crossentropy`** loss function, which is the standard choice for multi-label classification problems.\n",
        "\n",
        "* **Training:** Finally, the model is trained on the training data for 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWuiv-I8_zLA",
        "outputId": "f41d8fee-2c9b-41f8-a749-0895d64a4ca3"
      },
      "outputs": [],
      "source": [
        "# --- Assuming matrices X and y are ready from previous steps ---\n",
        "# X: Tokenized and padded sequence matrix\n",
        "# y: Binary label matrix\n",
        "\n",
        "# 1. Split data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Shape of training data:\", X_train.shape, y_train.shape)\n",
        "print(\"Shape of test data:\", X_test.shape, y_test.shape)\n",
        "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
        "\n",
        "\n",
        "# 2. Define model parameters\n",
        "# These values were calculated in previous steps\n",
        "# VOCAB_SIZE = len(vocab)\n",
        "# MAX_LEN = int(df['Sequence'].str.len().quantile(0.95))\n",
        "# NUM_LABELS = 1000\n",
        "EMBEDDING_DIM = 128  # Dimension of the vector learned for each amino acid\n",
        "\n",
        "\n",
        "# 3. Build model architecture\n",
        "print(\"Building 1D-CNN model...\")\n",
        "model = Sequential([\n",
        "    # Embedding layer: Converts each number (token) into a dense vector\n",
        "    Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_LEN),\n",
        "\n",
        "    # Convolutional layer to find patterns in the sequence\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "\n",
        "    # Pooling layer to reduce dimensionality and extract most important features\n",
        "    GlobalMaxPooling1D(),\n",
        "\n",
        "    # Fully connected layer for learning feature combinations\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5), # To prevent overfitting\n",
        "\n",
        "    # Output layer: Number of neurons equals the number of labels\n",
        "    # Sigmoid activation is essential for multi-label classification\n",
        "    Dense(NUM_LABELS, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 4. Compile the model\n",
        "# binary_crossentropy loss function is suitable for multi-label classification\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=[\n",
        "                  AUC(name='auc'),\n",
        "                  Precision(name='precision'),\n",
        "                  Recall(name='recall')\n",
        "              ])\n",
        "\n",
        "\n",
        "# 5. Train the model\n",
        "print(\"Starting model training process...\")\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(X_test, y_test)\n",
        ")\n",
        "\n",
        "print(\"\\nBase model training completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Baseline Model Evaluation\n",
        "\n",
        "After training, we evaluate the performance of our baseline CNN on the unseen **test set**. This step is crucial to understand how well the model generalizes to new data.\n",
        "\n",
        "The evaluation process includes:\n",
        "* **Generating Predictions:** The trained model predicts a probability score (between 0 and 1) for each of the 1,000 possible functions for every protein in the test set.\n",
        "* **Applying a Threshold:** We use a decision **threshold** to convert these probabilities into binary predictions (1 for \"has the function,\" 0 for \"does not\").\n",
        "* **Calculating Metrics:** We calculate two primary metrics to assess performance:\n",
        "    * **AUC (Area Under the Curve):** A threshold-independent metric that measures the model's overall ability to correctly rank positive labels higher than negative ones.\n",
        "    * **F1-Score:** A threshold-dependent metric that provides a balanced measure of a model's precision and recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmyV9EcQAetg",
        "outputId": "fc17328e-7d46-410f-a453-93786a090e18"
      },
      "outputs": [],
      "source": [
        "# 1. Get model predictions on test data\n",
        "y_pred_probs = model.predict(X_test)\n",
        "\n",
        "# 2. Convert probabilities to binary labels (0 and 1)\n",
        "THRESHOLD = 0.3\n",
        "y_pred = (y_pred_probs > THRESHOLD).astype(int)\n",
        "\n",
        "# 3. Calculate final metrics using 'micro' average to prevent errors\n",
        "print(f\"Prediction Threshold: {THRESHOLD}\\n\")\n",
        "\n",
        "# 'micro' averaging is robust to labels not present in the test set\n",
        "f1_micro = f1_score(y_test, y_pred, average='micro', zero_division=0)\n",
        "auc_micro = roc_auc_score(y_test, y_pred_probs, average='micro')\n",
        "\n",
        "print(f\"Final AUC Score (Micro): {auc_micro:.4f}\")\n",
        "print(f\"Final F1-Score (Micro): {f1_micro:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Advanced Model: Fine-Tuning a Transformer\n",
        "\n",
        "Now we move to our second approach: using a large, pre-trained **Transformer** model. Instead of building a model from scratch, we will use **transfer learning**. We will take a model that has already learned the \"language\" of proteins from millions of sequences and **fine-tune** it for our specific prediction task.\n",
        "\n",
        "This section covers loading the model and its tokenizer from the **Hugging Face** Hub.\n",
        "\n",
        "* **Model Selection:** We use **ESM-2** (`facebook/esm2_t6_8M_UR50D`), a powerful protein language model developed by Meta AI.\n",
        "* **Tokenizer and Model Loading:** We load the pre-trained model weights and the specific tokenizer that was trained with it.\n",
        "* **Model Customization:** We adapt the pre-trained model for our task by replacing its original classification layer with a new one designed for our 1,000-label, multi-label classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsAT4wD8A9gB",
        "outputId": "d83dce5d-e8b5-4493-f7e4-d97491c5a0a3"
      },
      "outputs": [],
      "source": [
        "# 1. Define the name of the model we want to use from Hugging Face\n",
        "# We are using a small version of ESM-2 called t6_8M, which is suitable for starting\n",
        "model_name = \"facebook/esm2_t6_8M_UR50D\"\n",
        "\n",
        "# 2. Load the specific tokenizer for this model\n",
        "# This tokenizer knows exactly how to prepare the protein sequence for ESM-2\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "NUM_LABELS = 1000\n",
        "# 3. Load the pre-trained model\n",
        "# We tell the model that we have a multi-label classification problem\n",
        "transformer_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=NUM_LABELS,  # This variable is our number of labels from the previous section (e.g., 1000)\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "transformer_model.to(device)\n",
        "\n",
        "print(f\"Model '{model_name}' successfully loaded and moved to device '{device}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Data Preparation for the Transformer\n",
        "\n",
        "The Hugging Face Transformer model requires data to be in a specific format for training. This step uses the model's own **tokenizer** and a custom **PyTorch Dataset** class to prepare the data.\n",
        "\n",
        "* **Tokenization:** We re-tokenize all protein sequences using the specific **ESM-2 tokenizer**. This process converts the sequences into numerical `input_ids` and also creates an `attention_mask`, which tells the model which tokens are real amino acids versus padding.\n",
        "* **PyTorch Dataset:** A custom `ProteinGODataset` class is created to package our tokenized inputs and binary labels together. This is the standard format required by the Hugging Face `Trainer` API for handling data during training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCyU23DuAsOI",
        "outputId": "02d0ab4b-3948-42f2-f11a-4fc7319433a2"
      },
      "outputs": [],
      "source": [
        "# --- Assuming 'df' and 'labels_df' DataFrames are ready from Section 2 ---\n",
        "# Reset indices to ensure consistency\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "labels_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# 1. Tokenize all sequences with the ESM-2 tokenizer\n",
        "print(\"Tokenizing sequences with Hugging Face tokenizer...\")\n",
        "tokenized_inputs = tokenizer( # Assuming 'tokenizer' is defined elsewhere (e.g., loaded ESM-2 tokenizer)\n",
        "    df['Sequence'].tolist(),\n",
        "    max_length=MAX_LEN,   # Using the same MAX_LEN as before (assuming it's defined)\n",
        "    padding='max_length', # Pad to maximum length\n",
        "    truncation=True,      # Truncate longer sequences\n",
        "    return_tensors='pt'   # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "# 2. Build a custom PyTorch Dataset class\n",
        "# This class prepares the data in the format required by Hugging Face Trainer\n",
        "class ProteinGODataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Returns a dictionary of required tensors for a single data sample\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        # Labels must be of float type\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# 3. Build training and test datasets\n",
        "# For a fair comparison, use the same random_state as before for data splitting\n",
        "indices = range(len(df))\n",
        "train_indices, test_indices = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert labels DataFrame to a NumPy array\n",
        "y_numpy = labels_df.values\n",
        "\n",
        "# Build training dataset\n",
        "train_dataset = ProteinGODataset(\n",
        "    {key: val[train_indices] for key, val in tokenized_inputs.items()},\n",
        "    y_numpy[train_indices]\n",
        ")\n",
        "\n",
        "# Build test dataset\n",
        "test_dataset = ProteinGODataset(\n",
        "    {key: val[test_indices] for key, val in tokenized_inputs.items()},\n",
        "    y_numpy[test_indices]\n",
        ")\n",
        "\n",
        "\n",
        "print(\"\\nData successfully converted to PyTorch Dataset format.\")\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of test samples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Transformer Fine-Tuning and Evaluation\n",
        "\n",
        "This is the final training step where we **fine-tune** the pre-trained ESM-2 model on our specific dataset. We use the high-level **`Trainer`** API from Hugging Face, which automates the entire process.\n",
        "\n",
        "* **Training Arguments:** We first define all the **hyperparameters** for our training run, such as the number of epochs, learning rate, and batch size, using the `TrainingArguments` class.\n",
        "* **Metrics Function:** A custom function, `compute_metrics`, is created to calculate the F1-Score and AUC on the validation set at the end of each epoch, allowing us to monitor the model's performance.\n",
        "* **Training:** The `Trainer` object brings together the model, data, and training arguments. The `trainer.train()` command launches the fine-tuning process.\n",
        "* **Final Evaluation:** After training is complete, we evaluate the best-performing version of the model on our unseen test set to get the final performance scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "lK-aiDCBZFWh",
        "outputId": "dddbdd47-e1de-4528-ead4-09eb3d695897"
      },
      "outputs": [],
      "source": [
        "# 1. Define a function to compute metrics during evaluation\n",
        "def compute_metrics(p):\n",
        "    # p is a tuple of predictions and labels\n",
        "    preds = p.predictions\n",
        "    labels = p.label_ids\n",
        "\n",
        "    # Apply sigmoid to get probabilities and find a threshold\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.Tensor(preds))\n",
        "\n",
        "    # We use the same 0.3 threshold as our baseline for a fair comparison\n",
        "    y_pred = (probs > 0.3).numpy().astype(int)\n",
        "    y_true = labels.astype(int)\n",
        "\n",
        "    # Calculate micro-averaged F1 and AUC scores\n",
        "    f1_micro = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
        "    auc_micro = roc_auc_score(y_true=y_true, y_score=probs, average='micro')\n",
        "\n",
        "    # Return metrics as a dictionary\n",
        "    return {\n",
        "        'f1_micro': f1_micro,\n",
        "        'auc_micro': auc_micro\n",
        "    }\n",
        "\n",
        "\n",
        "# 2. Define Training Arguments (Corrected Version)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=4,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    # --- RENAMED ARGUMENTS FOR OLDER VERSIONS ---\n",
        "    eval_strategy='epoch',     # Changed from evaluation_strategy\n",
        "    save_strategy='epoch',           # Changed from save_strategy\n",
        "    # ------------------------------------------\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1_micro',\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "\n",
        "# --- The rest of your code stays exactly the same ---\n",
        "# 3. Instantiate the Trainer\n",
        "trainer = Trainer(\n",
        "    model=transformer_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# 4. Start the fine-tuning process!\n",
        "print(\"Starting the fine-tuning process...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning complete!\")\n",
        "\n",
        "# 5. Evaluate the final best model\n",
        "final_evaluation = trainer.evaluate()\n",
        "print(\"\\n--- Final Evaluation of the Best Model ---\")\n",
        "print(final_evaluation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9. Final Analysis and Threshold Optimization\n",
        "After the models are trained, the final step is to perform a detailed analysis of their performance on the unseen test set. This involves generating predictions and then finding the optimal decision threshold for each model to maximize its F1-Score.\n",
        "\n",
        "This first code block handles generating the predictions from the fine-tuned Transformer model.\n",
        "\n",
        "* **Inference**: We use the `trainer.predict()` method to run the model on our test_dataset. This outputs raw, unnormalized scores known as logits.\n",
        "\n",
        "* **Probability Conversion**: A `sigmoid` function is applied to the logits to convert them into probabilities, which are values between 0 and 1 that can be used for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "BDaM8pWVlpDN",
        "outputId": "5e633807-e164-4c88-ac32-0eb15160692b"
      },
      "outputs": [],
      "source": [
        "# 1. Get the raw predictions from the fine-tuned trainer on the test_dataset\n",
        "raw_predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# The output contains the raw model outputs (logits)\n",
        "# We need to convert them to probabilities using a sigmoid function\n",
        "import torch\n",
        "\n",
        "y_pred_logits = raw_predictions.predictions\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "y_pred_probs_transformer = sigmoid(torch.Tensor(y_pred_logits)).numpy()\n",
        "\n",
        "print(\"Successfully generated probability predictions (y_pred_probs) from the Transformer model.\")\n",
        "print(\"Shape of the predictions:\", y_pred_probs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Finding the Optimal Decision Threshold\n",
        "\n",
        "The F1-Score is highly sensitive to the decision threshold used to convert probabilities into binary predictions. To find the best possible performance for each of our models, the following cells iterate through a range of thresholds and plot the results.\n",
        "\n",
        "This process is performed for both the **CNN baseline** and the **ESM-2 Transformer** to identify the **optimal threshold** that maximizes the F1-Score for each, ensuring a fair and complete comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1cou1ifB_GQ"
      },
      "outputs": [],
      "source": [
        "# --- Assuming y_test and y_pred_probs_transformer are ready from previous steps ---\n",
        "# y_pred_probs_transformer is the probability output from the transformer model\n",
        "\n",
        "# Test different thresholds from 0.1 to 0.5\n",
        "thresholds = np.arange(0.1, 0.5, 0.01)\n",
        "f1_scores = []\n",
        "\n",
        "for thresh in thresholds:\n",
        "    # Convert probabilities to binary predictions with the new threshold\n",
        "    y_pred_binary = (y_pred_probs_transformer > thresh).astype(int)\n",
        "    # Calculate F1-Score\n",
        "    f1 = f1_score(y_test, y_pred_binary, average='micro', zero_division=0)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "# Find the best threshold\n",
        "best_threshold_index = np.argmax(f1_scores)\n",
        "best_threshold = thresholds[best_threshold_index]\n",
        "best_f1_score = f1_scores[best_threshold_index]\n",
        "\n",
        "print(f\"Best threshold found: {best_threshold:.2f}\")\n",
        "print(f\"Best possible F1-Score: {best_f1_score:.4f}\")\n",
        "\n",
        "# Plot the graph for better analysis\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(thresholds, f1_scores, marker='o')\n",
        "plt.title('ESM-2 Model : F1-Score vs. Threshold')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('F1-Score (Micro)')\n",
        "plt.grid(True)\n",
        "plt.axvline(x=best_threshold, color='r', linestyle='--', label=f'Best Threshold = {best_threshold:.2f}')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWv_4VLbCeIQ"
      },
      "outputs": [],
      "source": [
        "# --- Assuming y_test and y_pred_probs are ready from previous steps ---\n",
        "# y_pred_probs is the probability output from the Baseline model\n",
        "\n",
        "# Test different thresholds from 0.1 to 0.5\n",
        "thresholds = np.arange(0.1, 0.5, 0.01)\n",
        "f1_scores = []\n",
        "\n",
        "for thresh in thresholds:\n",
        "    # Convert probabilities to binary predictions with the new threshold\n",
        "    y_pred_binary = (y_pred_probs_transformer > thresh).astype(int)\n",
        "    # Calculate F1-Score\n",
        "    f1 = f1_score(y_test, y_pred_binary, average='micro', zero_division=0)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "# Find the best threshold\n",
        "best_threshold_index = np.argmax(f1_scores)\n",
        "best_threshold = thresholds[best_threshold_index]\n",
        "best_f1_score = f1_scores[best_threshold_index]\n",
        "\n",
        "print(f\"Best threshold found: {best_threshold:.2f}\")\n",
        "print(f\"Best possible F1-Score: {best_f1_score:.4f}\")\n",
        "\n",
        "# Plot the graph for better analysis\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(thresholds, f1_scores, marker='o')\n",
        "plt.title('CNN Model : F1-Score vs. Threshold')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('F1-Score (Micro)')\n",
        "plt.grid(True)\n",
        "plt.axvline(x=best_threshold, color='r', linestyle='--', label=f'Best Threshold = {best_threshold:.2f}')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10. Conclusion and Final Thoughts\n",
        "\n",
        "This project successfully demonstrated an end-to-end workflow for predicting protein function from amino acid sequences using deep learning. We developed, trained, and rigorously compared two distinct models: a baseline 1D-CNN and a fine-tuned ESM-2 Transformer.\n",
        "\n",
        "After optimizing the decision threshold for both models, our final results showed that the **1D-CNN baseline model slightly outperformed the pre-trained ESM-2 Transformer** on both primary metrics.\n",
        "\n",
        "The key takeaway is a crucial lesson in applied machine learning: a well-designed, simpler architecture can be more effective than a large, complex pre-trained model, especially on specific, moderately-sized datasets. This result underscores the importance of **always establishing a strong baseline** and highlights that the most advanced model is not always the best solution for every problem."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
